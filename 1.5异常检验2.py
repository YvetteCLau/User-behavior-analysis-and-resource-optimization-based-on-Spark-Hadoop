from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, dayofweek
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from sklearn.ensemble import IsolationForest

# **1. åˆ›å»º SparkSessionï¼ˆå¢åŠ å†…å­˜ï¼‰**
spark = SparkSession.builder \
    .appName("Library Access Analysis") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# **2. è¯»å–é—¨ç¦æ•°æ®**
data_path = "file:///home/hadoop/Downloads/å›¾ä¹¦é¦†æ•°æ®/"
entry_logs = spark.read.csv(data_path + "entry_logs.csv", header=True, inferSchema=True)

# **3. å¤„ç†å…¥é¦†è®°å½•**
entry_logs = entry_logs.withColumn("Hour", hour(col("VisitTime"))) \
                       .withColumn("Weekday", dayofweek(col("VisitTime")))

# **4. å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼ˆIsolation Forestï¼‰**
# âœ… **é™åˆ¶æ•°æ®é‡ï¼Œé¿å… `.toPandas()` è¿‡è½½**
access_data = entry_logs.select("ID", "Hour").limit(10000).toPandas()

iso_forest = IsolationForest(contamination=0.05, random_state=42)
access_data["AnomalyScore"] = iso_forest.fit_predict(access_data[["Hour"]])

anomaly_df = spark.createDataFrame(access_data)

# **5. ç”¨æˆ·è¡Œä¸ºèšç±»ï¼ˆKMeansï¼‰**
assembler = VectorAssembler(inputCols=["Hour", "Weekday"], outputCol="features")
feature_data = assembler.transform(entry_logs.limit(10000))  # âœ… **é™åˆ¶æ•°æ®é‡**
kmeans = KMeans(featuresCol="features", k=3, seed=42)
user_clusters = kmeans.fit(feature_data).transform(feature_data)

# **6. å±•ç¤ºç»“æœ**
print("\nğŸ“Œ Top 10 - å¼‚å¸¸è¡Œä¸ºç”¨æˆ·ï¼š")
anomaly_df.show(10)

print("\nğŸ“Œ Top 10 - ç”¨æˆ·è¡Œä¸ºèšç±»ï¼š")
user_clusters.select("ID", "prediction").show(10)

# **7. å­˜å‚¨åˆ†æç»“æœ**
output_path = "file:///home/hadoop/Downloads/åˆ†æç»“æœ/"

anomaly_df.write.csv(output_path + "anomalies.csv", header=True, mode="overwrite")
user_clusters.select("ID", "prediction").write.csv(output_path + "user_clusters.csv", header=True, mode="overwrite")

# **8. å…³é—­ Spark**
spark.stop()
