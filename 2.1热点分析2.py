from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, regexp_extract, lower, trim, udf, when, concat_ws
from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, StopWordsRemover
from pyspark.ml.clustering import LDA
from pyspark.sql.types import StringType
import os
import jieba

# **1. åˆ›å»º SparkSession**
spark = SparkSession.builder \
    .appName("Library Homepage Analysis") \
    .config("spark.hadoop.fs.defaultFS", "file:///") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

# **2. è¯»å–å›¾ä¹¦é¦†ä¸»é¡µæ–‡ç« æ•°æ®**
articles_path = "file:///home/hadoop/Downloads/å›¾ä¹¦é¦†æ•°æ®/å›¾ä¹¦é¦†ä¸»é¡µæ•°æ®/å›¾ä¹¦é¦†ä¸»é¡µç½‘ç«™æ–‡ç« .csv"

if not os.path.exists(articles_path.replace("file://", "")):
    raise FileNotFoundError(f"âŒ æ–‡ç« æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {articles_path}")

articles = spark.read.csv(articles_path, header=True, inferSchema=True)

# **3. å¤„ç†åˆ—å**
articles = articles.toDF(*[c.strip() for c in articles.columns])

# **4. ç»Ÿä¸€åˆ—å**
articles = articles.select(
    col("æ ç›®").alias("Column"),
    col("æ ‡é¢˜").alias("Title"),
    col("é“¾æ¥").alias("URL"),
    col("å‘å¸ƒæ—¥æœŸ").alias("PublishDate"),
    col("è®¿é—®é‡").alias("Views"),
    col("æ–‡ç« å†…å®¹").alias("Content")
)

# **5. å¤„ç†æ–‡ç« å†…å®¹**
articles = articles.withColumn("Content", when(col("Content").isNull(), "").otherwise(col("Content")))
articles = articles.withColumn("Content", trim(col("Content")))
articles = articles.withColumn("LowerContent", lower(col("Content")))

# **6. ä½¿ç”¨ NLP æå–æ–‡ç« å…³é”®è¯**
def jieba_tokenizer(text):
    if text is None:
        return ""
    return " ".join(jieba.cut(text))

tokenizer_udf = udf(jieba_tokenizer, StringType())

articles = articles.withColumn("Tokens", tokenizer_udf(col("LowerContent")))

tokenizer = Tokenizer(inputCol="Tokens", outputCol="Words")
words_data = tokenizer.transform(articles)

remover = StopWordsRemover(inputCol="Words", outputCol="FilteredWords")
filtered_data = remover.transform(words_data)

vectorizer = CountVectorizer(inputCol="FilteredWords", outputCol="Features")
cv_model = vectorizer.fit(filtered_data)
vectorized_data = cv_model.transform(filtered_data)

idf = IDF(inputCol="Features", outputCol="TFIDF")
idf_model = idf.fit(vectorized_data)
tfidf_data = idf_model.transform(vectorized_data)

# **7. ä½¿ç”¨ LDA è¿›è¡Œä¸»é¢˜å»ºæ¨¡**
lda = LDA(k=5, featuresCol="TFIDF", seed=42)
lda_model = lda.fit(tfidf_data)
topics = lda_model.describeTopics(5)

# **8. ä¿®æ­£ LDA ç»“æœï¼Œè½¬æ¢æ•°ç»„åˆ—**
topics = topics.withColumn("termIndices", concat_ws(",", col("termIndices")))
topics = topics.withColumn("termWeights", concat_ws(",", col("termWeights")))

# **9. è¯»å–è®¿é—®æ—¥å¿—**
logs_path = "file:///home/hadoop/Downloads/å›¾ä¹¦é¦†æ•°æ®/å›¾ä¹¦é¦†ä¸»é¡µæ•°æ®/"

if not os.path.exists(logs_path.replace("file://", "")):
    raise FileNotFoundError(f"âŒ æ—¥å¿—æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {logs_path}")

log_files = [os.path.join(logs_path.replace("file://", ""), f) for f in os.listdir(logs_path.replace("file://", "")) if f.endswith("-result.log")]

if not log_files:
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥å¿—æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {logs_path}")

log_df = spark.read.text(log_files)

# **10. è§£æè®¿é—®è·¯å¾„**
log_df = log_df.withColumn("VisitURL", regexp_extract(col("value"), r'"(https://library.xmu.edu.cn.*?)"', 1))

# **11. ç»Ÿè®¡æ¯ç¯‡æ–‡ç« çš„è®¿é—®é‡**
visit_count = log_df.groupBy("VisitURL").agg(count("*").alias("VisitCount"))

# **12. ç»“åˆæ–‡ç« æ•°æ®ï¼Œè®¡ç®—æ–‡ç« çƒ­åº¦**
articles = articles.join(visit_count, articles["URL"] == visit_count["VisitURL"], "left").fillna(0)

# **13. å±•ç¤ºç»“æœ**
print("\nğŸ“Œ Top 10 æ–‡ç« è®¿é—®é‡ï¼š")
articles.select("Column", "Title", "Views", "VisitCount").orderBy(col("VisitCount").desc()).show(10)

print("\nğŸ“Œ æ–‡ç« ä¸»é¢˜åˆ†æï¼š")
topics.show(5, truncate=False)

# **14. å­˜å‚¨åˆ†æç»“æœ**
output_path = "file:///home/hadoop/Downloads/åˆ†æç»“æœ/"

articles.select("Column", "Title", "Views", "VisitCount").write.csv(output_path + "article_popularity.csv", header=True, mode="overwrite")
topics.write.csv(output_path + "article_topics.csv", header=True, mode="overwrite")

# **15. å…³é—­ Spark**
spark.stop()
