from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, udf, collect_set, month, year
from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, StopWordsRemover, StringIndexer
from pyspark.ml.recommendation import ALS
from pyspark.ml.clustering import LDA
from pyspark.sql.types import StringType, IntegerType, FloatType
import jieba
import numpy as np

# **1ï¸âƒ£ åˆ›å»º SparkSession**
spark = SparkSession.builder \
    .appName("Library User Interest & Recommendation") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# **2ï¸âƒ£ è¯»å–æ•°æ®**
data_path = "file:///home/hadoop/Downloads/å›¾ä¹¦é¦†æ•°æ®/"

reader_data = spark.read.csv(data_path + "è¯»è€…æ•°æ®_guid_utf8.csv", header=True, inferSchema=True)
borrow_logs = spark.read.csv(data_path + "å€Ÿé˜…æ•°æ®_guid_utf8.csv", header=True, inferSchema=True)
book_data = spark.read.csv(data_path + "å›¾ä¹¦æ•°æ®.csv", header=True, inferSchema=True)

# âœ… **æ£€æŸ¥ `borrow_logs` æ˜¯å¦ä¸ºç©º**
if borrow_logs.count() == 0:
    raise ValueError("âŒ borrow_logs æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®åŠ è½½ï¼")

# **3ï¸âƒ£ æ•°æ®æ¸…ç†**
reader_data = reader_data.select("ID", "GENDER", "ENROLLYEAR", "TYPE", "DEPARTMENT")
borrow_logs = borrow_logs.select("READERID", "BOOKID", "LENDDATE")
book_data = book_data.select("ID", "TITLE", "AUTHOR", "PUBLISHER", "YEAR", "CALLNO", "LANGUAGE", "DOCTYPE")

# **4ï¸âƒ£ å¤„ç†æ—¶é—´ä¿¡æ¯**
borrow_logs = borrow_logs.withColumn("Year", year(col("LENDDATE"))) \
                         .withColumn("Month", month(col("LENDDATE")))

# **5ï¸âƒ£ ç”¨æˆ·ç”»åƒåˆ†æ**
user_borrow_count = borrow_logs.groupBy("READERID").agg(count("*").alias("BorrowCount"))
user_category = borrow_logs.join(book_data, borrow_logs.BOOKID == book_data.ID, "left") \
                           .groupBy("READERID").agg(collect_set("DOCTYPE").alias("FavCategories"))
user_profile = user_borrow_count.join(user_category, "READERID", "left")

# **6ï¸âƒ£ NLP ä¸»é¢˜æŒ–æ˜**
def jieba_tokenizer(text):
    if text is None:
        return ""
    return " ".join(jieba.cut(text))

tokenizer_udf = udf(jieba_tokenizer, StringType())  
book_data = book_data.withColumn("Tokens", tokenizer_udf(col("TITLE")))  

# **7ï¸âƒ£ ä½¿ç”¨ Tokenizer**
tokenizer = Tokenizer(inputCol="Tokens", outputCol="Words")  
words_data = tokenizer.transform(book_data)

remover = StopWordsRemover(inputCol="Words", outputCol="FilteredWords")
filtered_data = remover.transform(words_data)

vectorizer = CountVectorizer(inputCol="FilteredWords", outputCol="Features")
cv_model = vectorizer.fit(filtered_data)
vectorized_data = cv_model.transform(filtered_data)

idf = IDF(inputCol="Features", outputCol="TFIDF")
idf_model = idf.fit(vectorized_data)
tfidf_data = idf_model.transform(vectorized_data)

# **8ï¸âƒ£ LDA ä¸»é¢˜å»ºæ¨¡**
lda = LDA(k=5, featuresCol="TFIDF", seed=42)
lda_model = lda.fit(tfidf_data)
topics = lda_model.describeTopics(5)

# **9ï¸âƒ£ è®­ç»ƒ ALS æ¨èç³»ç»Ÿ**
# âœ… **å°† `READERID` å’Œ `BOOKID` è½¬æ¢ä¸ºç´¢å¼•ç¼–å·**
reader_indexer = StringIndexer(inputCol="READERID", outputCol="READERID_Index")
book_indexer = StringIndexer(inputCol="BOOKID", outputCol="BOOKID_Index")

borrow_logs = reader_indexer.fit(borrow_logs).transform(borrow_logs)
borrow_logs = book_indexer.fit(borrow_logs).transform(borrow_logs)

# âœ… **æ„å»º ALS è®­ç»ƒæ•°æ®**
als_data = borrow_logs.groupBy("READERID_Index", "BOOKID_Index").agg(count("*").alias("BorrowCount"))

# âœ… **è¿‡æ»¤æ‰ NULL å€¼**
als_data = als_data.dropna(subset=["READERID_Index", "BOOKID_Index", "BorrowCount"])

# âœ… **æ£€æŸ¥ ALS è®­ç»ƒæ•°æ®æ˜¯å¦ä¸ºç©º**
if als_data.count() == 0:
    print("âš ï¸ ALS è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œå°è¯•æ‰“å° borrow_logs çš„å‰ 5 è¡Œï¼š")
    borrow_logs.show(5, truncate=False)
    raise ValueError("âŒ ALS è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ borrow_logs æ˜¯å¦æœ‰æ•°æ®ï¼")

# âœ… **è®­ç»ƒ ALS æ¨¡å‹**
als = ALS(userCol="READERID_Index", itemCol="BOOKID_Index", ratingCol="BorrowCount",
          coldStartStrategy="drop", implicitPrefs=True)
als_model = als.fit(als_data)

# **ğŸ”Ÿ ç”Ÿæˆæ¨è**
user_recommendations = als_model.recommendForAllUsers(5)

# **ğŸ“Œ è®¡ç®—åŸºäºå†…å®¹çš„æ¨è**
def cosine_similarity(vec1, vec2):
    vec1, vec2 = np.array(vec1), np.array(vec2)
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))

cosine_udf = udf(cosine_similarity, FloatType())

book_similarity = tfidf_data.alias("a").crossJoin(tfidf_data.alias("b")) \
    .withColumn("Similarity", cosine_udf(col("a.TFIDF"), col("b.TFIDF"))) \
    .filter(col("a.ID") != col("b.ID")) \
    .select(col("a.ID").alias("Book1"), col("b.ID").alias("Book2"), col("Similarity")) \
    .orderBy(col("Similarity").desc())

# **ğŸ“Œ åŠ¨æ€æ¨èï¼ˆæ—¶é—´è°ƒæ•´ï¼‰**
borrow_trends = borrow_logs.groupBy("Year", "Month", "BOOKID").agg(count("*").alias("BorrowCount"))
borrow_trends = borrow_trends.orderBy(col("Year").desc(), col("Month").desc())

# **ğŸ“Œ å±•ç¤ºç»“æœ**
print("ğŸ“Œ ç”¨æˆ·ç”»åƒï¼ˆTop 5ï¼‰ï¼š")
user_profile.show(5, truncate=False)

print("ğŸ“Œ ä¹¦ç±ä¸»é¢˜åˆ†æï¼ˆTop 5ï¼‰ï¼š")
topics.show(5, truncate=False)

print("ğŸ“Œ ååŒè¿‡æ»¤æ¨èï¼ˆTop 5 ç”¨æˆ·æ¨èï¼‰ï¼š")
user_recommendations.show(5, truncate=False)

print("ğŸ“Œ ä¹¦ç±ç›¸ä¼¼åº¦ï¼ˆTop 5 ç›¸ä¼¼ä¹¦ç±ï¼‰ï¼š")
book_similarity.show(5, truncate=False)

print("ğŸ“Œ å€Ÿé˜…è¶‹åŠ¿åˆ†æï¼ˆTop 5 æœ€æ–°è¶‹åŠ¿ï¼‰ï¼š")
borrow_trends.show(5, truncate=False)

# **ğŸ“Œ ä¿å­˜åˆ†æç»“æœ**
output_path = "file:///home/hadoop/Downloads/æ¨èç³»ç»Ÿç»“æœ/"

user_profile.write.csv(output_path + "user_profiles.csv", header=True, mode="overwrite")
user_recommendations.write.csv(output_path + "user_recommendations.csv", header=True, mode="overwrite")
book_similarity.write.csv(output_path + "book_similarity.csv", header=True, mode="overwrite")
borrow_trends.write.csv(output_path + "borrow_trends.csv", header=True, mode="overwrite")

# **ğŸ“Œ å…³é—­ Spark**
spark.stop()
