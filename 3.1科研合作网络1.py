from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, explode, collect_set
import networkx as nx
import pandas as pd
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random
import os

# **âœ… 1. åˆ›å»º SparkSession**
spark = SparkSession.builder \
    .appName("Academic Collaboration Network") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

# **âœ… 2. è¯»å–æ•°æ®**
data_path = "file:///home/hadoop/Downloads/å›¾ä¹¦é¦†æ•°æ®/å­¦è€…åº“æ•°æ®/"
output_path = "file:///home/hadoop/Downloads/ç§‘ç ”åˆä½œæ•°æ®/"

# **è‡ªåŠ¨è¯»å–æ‰€æœ‰ CSV æ–‡ä»¶**
article_data = None
csv_files = ["ArticleCn.csv", "ArticleEn.csv"]
for file in csv_files:
    file_path = data_path + file
    temp_df = spark.read.csv(file_path, header=True, inferSchema=True)
    
    if article_data is None:
        article_data = temp_df
    else:
        article_data = article_data.union(temp_df)

print(f"\nâœ… è¯»å– {len(csv_files)} ä¸ª CSV æ–‡ä»¶ï¼Œæ€»æ•°æ®é‡: {article_data.count()} è¡Œ")

# âœ… **éšæœºæŠ½æ · 2% çš„æ•°æ®**
article_data = article_data.sample(fraction=0.02, seed=42)
print(f"\nâœ… æŠ½æ ·åæ•°æ®é‡: {article_data.count()} è¡Œ")

# âœ… **æ‹†åˆ†ä½œè€…**
article_data = article_data.withColumn("Author", explode(split(col("ä½œè€…"), "[;,]")))
article_data = article_data.dropna(subset=["Author", "è®ºæ–‡åˆ†ç±»"])

# âœ… **ç»Ÿè®¡åˆä½œå…³ç³»**
coauthor_pairs = article_data.groupBy("è®ºæ–‡é¢˜å").agg(collect_set("Author").alias("Authors"))

# âœ… **æ„å»ºå­¦è€…åˆä½œç½‘ç»œ**
authors = article_data.select("Author").distinct().toPandas()
author_index = {name: idx for idx, name in enumerate(authors["Author"])}
edges = []

for row in coauthor_pairs.collect():
    author_list = list(set(row["Authors"]))
    for i in range(len(author_list)):
        for j in range(i + 1, len(author_list)):
            edges.append((author_index.get(author_list[i]), author_index.get(author_list[j])))

# âœ… **ä»…ä¿ç•™åˆä½œæ¬¡æ•° â‰¥5 çš„å­¦è€…**
collaboration_counts = {}
for a, b in edges:
    collaboration_counts[a] = collaboration_counts.get(a, 0) + 1
    collaboration_counts[b] = collaboration_counts.get(b, 0) + 1

filtered_edges = [(a, b) for a, b in edges if collaboration_counts[a] >= 5 and collaboration_counts[b] >= 5]
print("\nğŸ“Œ è¿‡æ»¤åè¾¹çš„æ•°é‡:", len(filtered_edges))

edge_index = torch.tensor(filtered_edges, dtype=torch.long).t().contiguous()

# âœ… **åˆ›å»º `Embedding`**
embedding_dim = 8  # é™ä½ç»´åº¦ï¼Œå‡å°‘è®¡ç®—é‡
x = torch.nn.Embedding(len(author_index), embedding_dim)

# **âœ… 3. æ„å»º PyTorch Geometric æ•°æ®**
data = Data(x=x.weight, edge_index=edge_index)

# **âœ… 4. å®šä¹‰ GCN é¢„æµ‹æ¨¡å‹**
class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# âœ… **è®­ç»ƒ GCN**
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("\nğŸ“Œ ä½¿ç”¨è®¾å¤‡:", device)

data = data.to(device)
model = GCN(in_channels=embedding_dim, hidden_channels=8, out_channels=embedding_dim).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

batch_size = 128  # è¿›ä¸€æ­¥é™ä½ batch_sizeï¼Œå‡å°‘è®¡ç®—å‹åŠ›
for epoch in range(30):  # è®­ç»ƒ 30 è½®ï¼ˆå‡å°‘è®­ç»ƒæ—¶é—´ï¼‰
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = F.mse_loss(out, data.x)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# âœ… **è·å–å­¦è€…åµŒå…¥**
model.eval()
embeddings = model(data.x, data.edge_index).cpu().detach().numpy()

# **âœ… 5. è®¡ç®—å­¦æœ¯é¢†åŸŸç›¸ä¼¼åº¦**
categories = article_data.select("Author", "è®ºæ–‡åˆ†ç±»").distinct().toPandas()
category_dict = categories.groupby("Author")["è®ºæ–‡åˆ†ç±»"].apply(lambda x: " ".join(x)).to_dict()

# âœ… **ä»…è®¡ç®—å‰ 2000 ä½å­¦è€…**
author_list = list(category_dict.keys())[:2000]
vectorizer = TfidfVectorizer()
category_matrix = vectorizer.fit_transform([category_dict[a] for a in author_list])
similarity_matrix = cosine_similarity(category_matrix)

# âœ… **ä¿å­˜åˆä½œæ¨èç»“æœ**
recommendations_df = pd.DataFrame(similarity_matrix, index=author_list, columns=author_list)
recommendations_df.to_csv(os.path.join(output_path.replace("file://", ""), "ç§‘ç ”åˆä½œæ¨è.csv"))

# **âœ… 6. è®¾ç½® Matplotlib ä¸­æ–‡å­—ä½“**
plt.rcParams["font.sans-serif"] = ["SimHei"]  # è§£å†³ä¸­æ–‡ä¹±ç 
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# **âœ… 7. å¯è§†åŒ–**
plt.figure(figsize=(12, 5))

# âœ… **åˆä½œç½‘ç»œ**
G = nx.Graph()
for a, b in filtered_edges:
    G.add_edge(authors["Author"].iloc[a], authors["Author"].iloc[b])

plt.subplot(1, 2, 1)
nx.draw(G, with_labels=False, node_size=10, edge_color="gray")
plt.title("å­¦è€…åˆä½œç½‘ç»œï¼ˆæŠ½æ ·ï¼‰")

# âœ… **å­¦æœ¯é¢†åŸŸç›¸ä¼¼åº¦çƒ­åŠ›å›¾**
plt.subplot(1, 2, 2)
sns.heatmap(similarity_matrix[:50, :50], cmap="Blues")
plt.title("å­¦æœ¯é¢†åŸŸç›¸ä¼¼åº¦çƒ­åŠ›å›¾ï¼ˆå‰ 50 åå­¦è€…ï¼‰")

plt.show()

# âœ… **ä¿å­˜åˆä½œç½‘ç»œæ•°æ®**
graph_df = pd.DataFrame(filtered_edges, columns=["Author_1", "Author_2"])
graph_df.to_csv(os.path.join(output_path.replace("file://", ""), "åˆä½œç½‘ç»œ.csv"), index=False)

# âœ… **å…³é—­ Spark**
spark.stop()
